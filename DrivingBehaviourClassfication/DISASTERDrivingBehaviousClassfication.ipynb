{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AccX      AccY      AccZ     GyroX     GyroY     GyroZ   Class\n",
      "0  0.000000  0.000000  0.000000  0.069951 -0.104000  0.082443  NORMAL\n",
      "1 -0.325915 -0.230327 -0.028581 -0.033627  0.030545  0.109615  NORMAL\n",
      "2 -0.119277 -0.026046  0.030865 -0.022838 -0.017455  0.071080  NORMAL\n",
      "3  0.148124 -0.048610  0.093468  0.082179 -0.017818  0.044402  NORMAL\n",
      "4  0.020407  0.165447 -0.009341  0.036145 -0.002182  0.044402  NORMAL\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "3639    0\n",
      "3640    0\n",
      "3641    0\n",
      "3642    0\n",
      "3643    0\n",
      "Name: Class, Length: 3644, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_46020\\334160662.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_data_df['Class'] = train_data_df['Class'].replace(class_mapping)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_46020\\334160662.py:46: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_data_df['Class'] = test_data_df['Class'].replace(class_mapping)\n"
     ]
    }
   ],
   "source": [
    "#impoering the libraries and moduels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset ,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "#importing and cleaning the dataset\n",
    "test_data_df = pd.read_csv(r\"D:\\Datasets\\Kaggle\\Driving _behaviour_dataset\\test_motion_data.csv\")\n",
    "train_data_df = pd.read_csv(r\"D:\\Datasets\\Kaggle\\Driving _behaviour_dataset\\train_motion_data.csv\")\n",
    "original_test_data_df = test_data_df.copy()\n",
    "original_train_data_df = train_data_df.copy()\n",
    "test_data_df.dropna(inplace=True)\n",
    "train_data_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "#normalizing \n",
    "for column in train_data_df:\n",
    "    if train_data_df[column].dtype in  ['int','float']:\n",
    "        train_data_df[column] = train_data_df[column]/train_data_df[column].abs().max()\n",
    "\n",
    "for column in test_data_df:\n",
    "    if test_data_df[column].dtype in ['int','float']:\n",
    "        test_data_df[column] = test_data_df[column]/test_data_df[column].abs().max()\n",
    "\n",
    "#omitting out the timestamp column cuz its irrelevant\n",
    "test_data_df.drop(['Timestamp'], axis=1, inplace=True)\n",
    "train_data_df.drop(['Timestamp'], axis=1, inplace=True)\n",
    "print(train_data_df.head())\n",
    "\n",
    "#replacing the class column with integeres witht he help of mapping\n",
    "class_mapping = {\n",
    "    'SLOW' : 0,\n",
    "    'NORMAL' : 1,\n",
    "    'AGGRESSIVE' : 2\n",
    "}\n",
    "train_data_df['Class'] = train_data_df['Class'].replace(class_mapping)\n",
    "test_data_df['Class'] = test_data_df['Class'].replace(class_mapping)\n",
    "print(train_data_df['Class'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data ##\n",
    "Looks like I was wrong to feed the data with basic processing as it is. This data shows the sequential information of a person/s driving the car, so it is better to take the average and \n",
    "standard deviation into accouint.\n",
    "Potentially, I can also try this with a CNN + LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AccX      AccY      AccZ     GyroX     GyroY     GyroZ  Class  \\\n",
      "0  0.000000  0.000000  0.000000  0.069951 -0.104000  0.082443      1   \n",
      "1 -0.325915 -0.230327 -0.028581 -0.033627  0.030545  0.109615      1   \n",
      "2 -0.119277 -0.026046  0.030865 -0.022838 -0.017455  0.071080      1   \n",
      "3  0.148124 -0.048610  0.093468  0.082179 -0.017818  0.044402      1   \n",
      "4  0.020407  0.165447 -0.009341  0.036145 -0.002182  0.044402      1   \n",
      "\n",
      "   AccX_mean  AccX_std  AccY_mean  AccY_std  GyroX_mean  GyroX_std  \\\n",
      "0        NaN       NaN        NaN       NaN         NaN        NaN   \n",
      "1        NaN       NaN        NaN       NaN         NaN        NaN   \n",
      "2        NaN       NaN        NaN       NaN         NaN        NaN   \n",
      "3        NaN       NaN        NaN       NaN         NaN        NaN   \n",
      "4        NaN       NaN        NaN       NaN         NaN        NaN   \n",
      "\n",
      "   GyroY_mean  GyroY_std  GyroZ_mean  GyroZ_std  \n",
      "0         NaN        NaN         NaN        NaN  \n",
      "1         NaN        NaN         NaN        NaN  \n",
      "2         NaN        NaN         NaN        NaN  \n",
      "3         NaN        NaN         NaN        NaN  \n",
      "4         NaN        NaN         NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "## Adding columns for mean and standard deviation across every 50 samples with rolling() from pandas\n",
    "window = 50\n",
    "\n",
    "features = train_data_df[['AccX','AccY','GyroX','GyroY','GyroZ']].rolling(window)\n",
    "\n",
    "train_data_df['AccX_mean'] = features['AccX'].mean()\n",
    "train_data_df['AccX_std'] = features['AccX'].std()\n",
    "train_data_df['AccY_mean'] = features['AccY'].mean()\n",
    "train_data_df['AccY_std'] = features['AccY'].std()\n",
    "train_data_df['GyroX_mean'] = features['GyroX'].mean()\n",
    "train_data_df['GyroX_std'] = features['GyroX'].std()\n",
    "train_data_df['GyroY_mean'] = features['GyroY'].mean()\n",
    "train_data_df['GyroY_std'] = features['GyroY'].std()\n",
    "train_data_df['GyroZ_mean'] = features['GyroZ'].mean()\n",
    "train_data_df['GyroZ_std'] = features['GyroZ'].std()\n",
    "print(train_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Workflow ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data cleaning - done\n",
    "2. Dataset setup: most conceptual i believe\n",
    "3. DataLoader magik\n",
    "4. Design the model, ooh gawd\n",
    "5. Training loop ooh lawd have mercy\n",
    "6. Test the model, ** insert confused cat meme here **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Dataset magik\n",
    "\n",
    "#converting to numpy arrays\n",
    "X = np.array(train_data_df.iloc[:,:-1])\n",
    "Y = np.array(train_data_df.iloc[:,-1])\n",
    "X_test = np.array(test_data_df.iloc[:,:-1])\n",
    "Y_test = np.array(test_data_df.iloc[:,-1])\n",
    "\n",
    "#creating a sweet validation set from train set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, train_size=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 6 features, but StandardScaler is expecting 16 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m X_train = scaler.transform(X_train)\n\u001b[32m     10\u001b[39m X_val = scaler.transform(X_val)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_test = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\Python learnen materials\\Projects\\ProjecrtEnv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\Python learnen materials\\Projects\\ProjecrtEnv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\Python learnen materials\\Projects\\ProjecrtEnv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Coding\\Python learnen materials\\Projects\\ProjecrtEnv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 6 features, but StandardScaler is expecting 16 features as input."
     ]
    }
   ],
   "source": [
    "##Apparently, the data is very sporadic with very less proportional changes happening across the axes.\n",
    "##Implementing standardscaler from sklearn library,I have no idea what it is. I am just going to ask ai about this and figure it out.\n",
    "##No code generation with AI though, that would defeat the purpose.\n",
    "##here is to more focus on data pre processing cheerios!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset(X_train, Y_train)\n",
    "validation_data = dataset(X_val, Y_val)\n",
    "test_data = dataset(X_test, Y_test)\n",
    "\n",
    "#Loading the dataset into dataloader\n",
    "train_dataloader = DataLoader(training_data, batch_size=8, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designing the model\n",
    "class DriveModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DriveModel, self). __init__()\n",
    "        #adding more widerlayers cuz the accuracy was shit \n",
    "        #adding dropout layers cuz the model aint stronk yet\n",
    "        self.inputlayer = nn.Linear(X.shape[1], 128)\n",
    "        self.reLu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.layer3 = nn.Linear(64,32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.output_layer = nn.Linear(32,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inputlayer(x)\n",
    "        x = self.reLu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu3(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "    \n",
    "model = DriveModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr = 1e-3, weight_decay= 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "Epoch: [1/80]\n",
      "Training loss(avg): 1.092067061221763 || Training acc(avg): 0.3801\n",
      "Validation loss(avg): 1.06953311096067 || training acc(avg): 0.4252\n",
      "************************\n",
      "Epoch: [2/80]\n",
      "Training loss(avg): 1.066876772821766 || Training acc(avg): 0.4182\n",
      "Validation loss(avg): 1.0660719644764196 || training acc(avg): 0.4129\n",
      "************************\n",
      "Epoch: [3/80]\n",
      "Training loss(avg): 1.0584684396443302 || Training acc(avg): 0.4268\n",
      "Validation loss(avg): 1.0529779484090598 || training acc(avg): 0.4348\n",
      "************************\n",
      "Epoch: [4/80]\n",
      "Training loss(avg): 1.049496455225226 || Training acc(avg): 0.4364\n",
      "Validation loss(avg): 1.0559558687002764 || training acc(avg): 0.4431\n",
      "************************\n",
      "Epoch: [5/80]\n",
      "Training loss(avg): 1.0466925514887457 || Training acc(avg): 0.4391\n",
      "Validation loss(avg): 1.0608365548693615 || training acc(avg): 0.4225\n",
      "************************\n",
      "Epoch: [6/80]\n",
      "Training loss(avg): 1.0456776147019373 || Training acc(avg): 0.4398\n",
      "Validation loss(avg): 1.057604022000147 || training acc(avg): 0.4417\n",
      "************************\n",
      "Epoch: [7/80]\n",
      "Training loss(avg): 1.0457101467537553 || Training acc(avg): 0.4443\n",
      "Validation loss(avg): 1.0623816588650579 || training acc(avg): 0.4198\n",
      "************************\n",
      "Epoch: [8/80]\n",
      "Training loss(avg): 1.0377613322375572 || Training acc(avg): 0.4515\n",
      "Validation loss(avg): 1.0652604802795078 || training acc(avg): 0.4307\n",
      "************************\n",
      "Epoch: [9/80]\n",
      "Training loss(avg): 1.041000653782936 || Training acc(avg): 0.4535\n",
      "Validation loss(avg): 1.0558308809995651 || training acc(avg): 0.4376\n",
      "************************\n",
      "Epoch: [10/80]\n",
      "Training loss(avg): 1.0360593122978734 || Training acc(avg): 0.4467\n",
      "Validation loss(avg): 1.0613636342079744 || training acc(avg): 0.4129\n",
      "************************\n",
      "Epoch: [11/80]\n",
      "Training loss(avg): 1.033354355537728 || Training acc(avg): 0.4504\n",
      "Validation loss(avg): 1.058105253007101 || training acc(avg): 0.4252\n",
      "************************\n",
      "Epoch: [12/80]\n",
      "Training loss(avg): 1.0333155127420817 || Training acc(avg): 0.4545\n",
      "Validation loss(avg): 1.0559324563845345 || training acc(avg): 0.4390\n",
      "************************\n",
      "Epoch: [13/80]\n",
      "Training loss(avg): 1.029081615846451 || Training acc(avg): 0.4563\n",
      "Validation loss(avg): 1.0577873587608337 || training acc(avg): 0.4335\n",
      "************************\n",
      "Epoch: [14/80]\n",
      "Training loss(avg): 1.0279399878358189 || Training acc(avg): 0.4655\n",
      "Validation loss(avg): 1.0625997548517974 || training acc(avg): 0.4060\n",
      "************************\n",
      "Epoch: [15/80]\n",
      "Training loss(avg): 1.0318192320327235 || Training acc(avg): 0.4497\n",
      "Validation loss(avg): 1.0565512445957765 || training acc(avg): 0.4143\n",
      "************************\n",
      "Epoch: [16/80]\n",
      "Training loss(avg): 1.0271796157915298 || Training acc(avg): 0.4590\n",
      "Validation loss(avg): 1.058431350018667 || training acc(avg): 0.4348\n",
      "************************\n",
      "Epoch: [17/80]\n",
      "Training loss(avg): 1.0230748369269176 || Training acc(avg): 0.4672\n",
      "Validation loss(avg): 1.0665700500426085 || training acc(avg): 0.4335\n",
      "************************\n",
      "Epoch: [18/80]\n",
      "Training loss(avg): 1.0241621788233928 || Training acc(avg): 0.4710\n",
      "Validation loss(avg): 1.0592715435701867 || training acc(avg): 0.4335\n",
      "************************\n",
      "Epoch: [19/80]\n",
      "Training loss(avg): 1.0206796113758871 || Training acc(avg): 0.4672\n",
      "Validation loss(avg): 1.0582437631876573 || training acc(avg): 0.4294\n",
      "************************\n",
      "Epoch: [20/80]\n",
      "Training loss(avg): 1.0196146981356895 || Training acc(avg): 0.4748\n",
      "Validation loss(avg): 1.0647684193175773 || training acc(avg): 0.4280\n",
      "************************\n",
      "Epoch: [21/80]\n",
      "Training loss(avg): 1.0194639671338748 || Training acc(avg): 0.4600\n",
      "Validation loss(avg): 1.0698240464148314 || training acc(avg): 0.4198\n",
      "************************\n",
      "Epoch: [22/80]\n",
      "Training loss(avg): 1.0233566292344707 || Training acc(avg): 0.4679\n",
      "Validation loss(avg): 1.053412025389464 || training acc(avg): 0.4431\n",
      "************************\n",
      "Epoch: [23/80]\n",
      "Training loss(avg): 1.0222521559832847 || Training acc(avg): 0.4710\n",
      "Validation loss(avg): 1.0613869208356608 || training acc(avg): 0.4362\n",
      "************************\n",
      "Epoch: [24/80]\n",
      "Training loss(avg): 1.0145357763930543 || Training acc(avg): 0.4768\n",
      "Validation loss(avg): 1.0669453377309053 || training acc(avg): 0.4266\n",
      "************************\n",
      "Epoch: [25/80]\n",
      "Training loss(avg): 1.0140420360107945 || Training acc(avg): 0.4758\n",
      "Validation loss(avg): 1.065578047995982 || training acc(avg): 0.4102\n",
      "************************\n",
      "Epoch: [26/80]\n",
      "Training loss(avg): 1.0146100614168874 || Training acc(avg): 0.4631\n",
      "Validation loss(avg): 1.0629774105289709 || training acc(avg): 0.4472\n",
      "************************\n",
      "Epoch: [27/80]\n",
      "Training loss(avg): 1.0171087178465439 || Training acc(avg): 0.4724\n",
      "Validation loss(avg): 1.0674469600553098 || training acc(avg): 0.4239\n",
      "************************\n",
      "Epoch: [28/80]\n",
      "Training loss(avg): 1.01232186376232 || Training acc(avg): 0.4792\n",
      "Validation loss(avg): 1.0705537193495294 || training acc(avg): 0.4239\n",
      "************************\n",
      "Epoch: [29/80]\n",
      "Training loss(avg): 1.0113630013923123 || Training acc(avg): 0.4724\n",
      "Validation loss(avg): 1.0640453093725701 || training acc(avg): 0.4348\n",
      "************************\n",
      "Epoch: [30/80]\n",
      "Training loss(avg): 1.016992066494406 || Training acc(avg): 0.4786\n",
      "Validation loss(avg): 1.0662462361480878 || training acc(avg): 0.4376\n",
      "************************\n",
      "Epoch: [31/80]\n",
      "Training loss(avg): 1.0135629185258526 || Training acc(avg): 0.4782\n",
      "Validation loss(avg): 1.0570445177347765 || training acc(avg): 0.4362\n",
      "************************\n",
      "Epoch: [32/80]\n",
      "Training loss(avg): 1.01150801100143 || Training acc(avg): 0.4755\n",
      "Validation loss(avg): 1.0619978911202888 || training acc(avg): 0.4225\n",
      "************************\n",
      "Epoch: [33/80]\n",
      "Training loss(avg): 1.0116464005757684 || Training acc(avg): 0.4799\n",
      "Validation loss(avg): 1.0641769358645314 || training acc(avg): 0.4252\n",
      "************************\n",
      "Epoch: [34/80]\n",
      "Training loss(avg): 1.0060634456268729 || Training acc(avg): 0.4834\n",
      "Validation loss(avg): 1.0612973136746364 || training acc(avg): 0.4307\n",
      "************************\n",
      "Epoch: [35/80]\n",
      "Training loss(avg): 1.006218197900955 || Training acc(avg): 0.4840\n",
      "Validation loss(avg): 1.0675178759771844 || training acc(avg): 0.4403\n",
      "************************\n",
      "Epoch: [36/80]\n",
      "Training loss(avg): 1.007463613928181 || Training acc(avg): 0.4768\n",
      "Validation loss(avg): 1.07010764013166 || training acc(avg): 0.4321\n",
      "************************\n",
      "Epoch: [37/80]\n",
      "Training loss(avg): 1.0029416877929478 || Training acc(avg): 0.4751\n",
      "Validation loss(avg): 1.0673541970874951 || training acc(avg): 0.4156\n",
      "************************\n",
      "Epoch: [38/80]\n",
      "Training loss(avg): 1.0006060214891825 || Training acc(avg): 0.4913\n",
      "Validation loss(avg): 1.075102651896684 || training acc(avg): 0.4403\n",
      "************************\n",
      "Epoch: [39/80]\n",
      "Training loss(avg): 0.9939946860483247 || Training acc(avg): 0.4895\n",
      "Validation loss(avg): 1.0714732116979102 || training acc(avg): 0.4376\n",
      "************************\n",
      "Epoch: [40/80]\n",
      "Training loss(avg): 1.0021428299276796 || Training acc(avg): 0.4806\n",
      "Validation loss(avg): 1.0683852395285731 || training acc(avg): 0.4266\n",
      "************************\n",
      "Epoch: [41/80]\n",
      "Training loss(avg): 0.9994920810608015 || Training acc(avg): 0.4906\n",
      "Validation loss(avg): 1.0713265369767728 || training acc(avg): 0.4239\n",
      "************************\n",
      "Epoch: [42/80]\n",
      "Training loss(avg): 1.0066012384140328 || Training acc(avg): 0.4803\n",
      "Validation loss(avg): 1.0727777928113937 || training acc(avg): 0.4403\n",
      "************************\n",
      "Epoch: [43/80]\n",
      "Training loss(avg): 0.9962343568671239 || Training acc(avg): 0.4861\n",
      "Validation loss(avg): 1.075077389245448 || training acc(avg): 0.4252\n",
      "************************\n",
      "Epoch: [44/80]\n",
      "Training loss(avg): 1.003727893143484 || Training acc(avg): 0.4933\n",
      "Validation loss(avg): 1.0757278275230657 || training acc(avg): 0.4390\n",
      "************************\n",
      "Epoch: [45/80]\n",
      "Training loss(avg): 0.9940167278459627 || Training acc(avg): 0.4926\n",
      "Validation loss(avg): 1.0665904685207035 || training acc(avg): 0.4335\n",
      "************************\n",
      "Epoch: [46/80]\n",
      "Training loss(avg): 0.9968831400348716 || Training acc(avg): 0.4830\n",
      "Validation loss(avg): 1.0792394487754158 || training acc(avg): 0.4321\n",
      "************************\n",
      "Epoch: [47/80]\n",
      "Training loss(avg): 0.9936542091304309 || Training acc(avg): 0.4991\n",
      "Validation loss(avg): 1.074088065520577 || training acc(avg): 0.4321\n",
      "************************\n",
      "Epoch: [48/80]\n",
      "Training loss(avg): 0.9913400011519863 || Training acc(avg): 0.4978\n",
      "Validation loss(avg): 1.0918180586203285 || training acc(avg): 0.4321\n",
      "************************\n",
      "Epoch: [49/80]\n",
      "Training loss(avg): 0.9972758208235649 || Training acc(avg): 0.4957\n",
      "Validation loss(avg): 1.0762152347875678 || training acc(avg): 0.4403\n",
      "************************\n",
      "Epoch: [50/80]\n",
      "Training loss(avg): 0.9884347667432811 || Training acc(avg): 0.4947\n",
      "Validation loss(avg): 1.091592299549476 || training acc(avg): 0.4307\n",
      "************************\n",
      "Epoch: [51/80]\n",
      "Training loss(avg): 0.9916084981944463 || Training acc(avg): 0.4913\n",
      "Validation loss(avg): 1.1019434384677722 || training acc(avg): 0.4390\n",
      "************************\n",
      "Epoch: [52/80]\n",
      "Training loss(avg): 0.9905537582423589 || Training acc(avg): 0.4991\n",
      "Validation loss(avg): 1.0883393637512042 || training acc(avg): 0.4115\n",
      "************************\n",
      "Epoch: [53/80]\n",
      "Training loss(avg): 0.9874133438280184 || Training acc(avg): 0.4964\n",
      "Validation loss(avg): 1.0753118376369062 || training acc(avg): 0.4307\n",
      "************************\n",
      "Epoch: [54/80]\n",
      "Training loss(avg): 0.9975180346671849 || Training acc(avg): 0.4882\n",
      "Validation loss(avg): 1.079504975806112 || training acc(avg): 0.4129\n",
      "************************\n",
      "Epoch: [55/80]\n",
      "Training loss(avg): 0.9833511119019496 || Training acc(avg): 0.5081\n",
      "Validation loss(avg): 1.0856190654246702 || training acc(avg): 0.4005\n",
      "************************\n",
      "Epoch: [56/80]\n",
      "Training loss(avg): 0.9830177326724954 || Training acc(avg): 0.5094\n",
      "Validation loss(avg): 1.0828688961008321 || training acc(avg): 0.4074\n",
      "************************\n",
      "Epoch: [57/80]\n",
      "Training loss(avg): 0.9858133502202491 || Training acc(avg): 0.5043\n",
      "Validation loss(avg): 1.0917588556590287 || training acc(avg): 0.4348\n",
      "************************\n",
      "Epoch: [58/80]\n",
      "Training loss(avg): 0.9888864656017251 || Training acc(avg): 0.5053\n",
      "Validation loss(avg): 1.0993768754212752 || training acc(avg): 0.4184\n",
      "************************\n",
      "Epoch: [59/80]\n",
      "Training loss(avg): 0.9840066147177187 || Training acc(avg): 0.5053\n",
      "Validation loss(avg): 1.129419611847919 || training acc(avg): 0.4225\n",
      "************************\n",
      "Epoch: [60/80]\n",
      "Training loss(avg): 0.9881908594745479 || Training acc(avg): 0.5019\n",
      "Validation loss(avg): 1.0832456583562105 || training acc(avg): 0.4266\n",
      "************************\n",
      "Epoch: [61/80]\n",
      "Training loss(avg): 0.9789711829734176 || Training acc(avg): 0.5094\n",
      "Validation loss(avg): 1.082160546079926 || training acc(avg): 0.4266\n",
      "************************\n",
      "Epoch: [62/80]\n",
      "Training loss(avg): 0.9883592373704257 || Training acc(avg): 0.5043\n",
      "Validation loss(avg): 1.0877591928710109 || training acc(avg): 0.4362\n",
      "************************\n",
      "Epoch: [63/80]\n",
      "Training loss(avg): 0.9803303329911951 || Training acc(avg): 0.5132\n",
      "Validation loss(avg): 1.0998353569403938 || training acc(avg): 0.4252\n",
      "************************\n",
      "Epoch: [64/80]\n",
      "Training loss(avg): 0.9748978618073136 || Training acc(avg): 0.5146\n",
      "Validation loss(avg): 1.1014433710471443 || training acc(avg): 0.4184\n",
      "************************\n",
      "Epoch: [65/80]\n",
      "Training loss(avg): 0.9831970616562725 || Training acc(avg): 0.5070\n",
      "Validation loss(avg): 1.0949467556632084 || training acc(avg): 0.4294\n",
      "************************\n",
      "Epoch: [66/80]\n",
      "Training loss(avg): 0.980636085549446 || Training acc(avg): 0.5139\n",
      "Validation loss(avg): 1.1020474161790765 || training acc(avg): 0.4129\n",
      "************************\n",
      "Epoch: [67/80]\n",
      "Training loss(avg): 0.9771908013787988 || Training acc(avg): 0.5043\n",
      "Validation loss(avg): 1.0901398581007253 || training acc(avg): 0.4143\n",
      "************************\n",
      "Epoch: [68/80]\n",
      "Training loss(avg): 0.9798622173805759 || Training acc(avg): 0.5129\n",
      "Validation loss(avg): 1.0970132422188055 || training acc(avg): 0.4239\n",
      "************************\n",
      "Epoch: [69/80]\n",
      "Training loss(avg): 0.9708281252482166 || Training acc(avg): 0.5132\n",
      "Validation loss(avg): 1.1128477784602537 || training acc(avg): 0.4198\n",
      "************************\n",
      "Epoch: [70/80]\n",
      "Training loss(avg): 0.981392641916667 || Training acc(avg): 0.4998\n",
      "Validation loss(avg): 1.091903952476771 || training acc(avg): 0.4239\n",
      "************************\n",
      "Epoch: [71/80]\n",
      "Training loss(avg): 0.9823791970128882 || Training acc(avg): 0.5033\n",
      "Validation loss(avg): 1.0966300407181615 || training acc(avg): 0.4294\n",
      "************************\n",
      "Epoch: [72/80]\n",
      "Training loss(avg): 0.9735075777524138 || Training acc(avg): 0.5286\n",
      "Validation loss(avg): 1.08946293592453 || training acc(avg): 0.4184\n",
      "************************\n",
      "Epoch: [73/80]\n",
      "Training loss(avg): 0.9758332984088218 || Training acc(avg): 0.5160\n",
      "Validation loss(avg): 1.0951942030502402 || training acc(avg): 0.4156\n",
      "************************\n",
      "Epoch: [74/80]\n",
      "Training loss(avg): 0.9786258663216683 || Training acc(avg): 0.5160\n",
      "Validation loss(avg): 1.1001224226277808 || training acc(avg): 0.4156\n",
      "************************\n",
      "Epoch: [75/80]\n",
      "Training loss(avg): 0.969170767477114 || Training acc(avg): 0.5132\n",
      "Validation loss(avg): 1.1104462976041047 || training acc(avg): 0.4431\n",
      "************************\n",
      "Epoch: [76/80]\n",
      "Training loss(avg): 0.9705990670478507 || Training acc(avg): 0.5136\n",
      "Validation loss(avg): 1.1094563979169596 || training acc(avg): 0.4102\n",
      "************************\n",
      "Epoch: [77/80]\n",
      "Training loss(avg): 0.9626995042578815 || Training acc(avg): 0.5307\n",
      "Validation loss(avg): 1.1100789042918577 || training acc(avg): 0.4184\n",
      "************************\n",
      "Epoch: [78/80]\n",
      "Training loss(avg): 0.9790764454292924 || Training acc(avg): 0.5005\n",
      "Validation loss(avg): 1.1055508210607197 || training acc(avg): 0.4266\n",
      "************************\n",
      "Epoch: [79/80]\n",
      "Training loss(avg): 0.9692030586608469 || Training acc(avg): 0.5190\n",
      "Validation loss(avg): 1.1153275571439578 || training acc(avg): 0.3992\n",
      "************************\n",
      "Epoch: [80/80]\n",
      "Training loss(avg): 0.9688104222898614 || Training acc(avg): 0.5156\n",
      "Validation loss(avg): 1.1078775965649148 || training acc(avg): 0.4390\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "epochs = 80\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        batch_loss = loss_fn(output, labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "        #accuracry counter\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "\n",
    "        #counting the the predictions that match the labels\n",
    "        running_accuracy += torch.sum(predicted_classes == labels).item()\n",
    "\n",
    "    avg_train_loss = running_loss/len(train_dataloader)\n",
    "    avg_train_acc = running_accuracy/len(train_dataloader.dataset)\n",
    "\n",
    "    #Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    val_running_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for val_input, val_labels in validation_dataloader:\n",
    "            val_input = val_input.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_output = model(val_input)\n",
    "            val_batch_loss = loss_fn(val_output, val_labels)\n",
    "\n",
    "            val_running_loss += val_batch_loss.item()\n",
    "\n",
    "            #accuracy for val set\n",
    "            _, val_predicted_classes = torch.max(val_output, 1)\n",
    "            val_running_accuracy += torch.sum(val_predicted_classes == val_labels).item()\n",
    "\n",
    "    avg_val_loss = val_running_loss/len(validation_dataloader)\n",
    "    avg_val_acc = val_running_accuracy/len(validation_dataloader.dataset)\n",
    "\n",
    "    print('**' * 12)\n",
    "\n",
    "    print(f\"Epoch: [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Training loss(avg): {avg_train_loss} || Training acc(avg): {avg_train_acc:.4f}\")\n",
    "    print(f\"Validation loss(avg): {avg_val_loss} || training acc(avg): {avg_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjecrtEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
