{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_41188\\1314476459.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df['Class'] = train_df['Class'].replace(class_mapping)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_41188\\1314476459.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df['Class'] = test_df['Class'].replace(class_mapping)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Datasets\\Kaggle\\Driving _behaviour_dataset\\train_motion_data.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Datasets\\Kaggle\\Driving _behaviour_dataset\\test_motion_data.csv\")\n",
    "og_train_df = train_df.copy()\n",
    "og_test_df = test_df.copy()\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df.drop(columns= 'Timestamp')\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df.drop(columns= 'Timestamp')\n",
    "\n",
    "class_mapping = {\n",
    "    'SLOW' : 0,\n",
    "    'NORMAL' : 1,\n",
    "    'AGGRESSIVE' : 2\n",
    "}\n",
    "\n",
    "train_df['Class'] = train_df['Class'].replace(class_mapping)\n",
    "test_df['Class'] = test_df['Class'].replace(class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns='Class')\n",
    "Y = train_df['Class']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, train_size=0.8, stratify=Y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "def add_rolling_feats(df, window = 50):\n",
    "    columns = ['AccX','AccY', 'AccZ','GyroX', 'GyroY', 'GyroZ']\n",
    "    for col in columns:\n",
    "        df[f\"{col}_mean\"] = df[col].rolling(window).mean()\n",
    "        df[f\"{col}_std\"] = df[col].rolling(window).std()\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([X_train, Y_train], axis=1)\n",
    "val_full = pd.concat([X_val,Y_val], axis= 1)\n",
    "\n",
    "train_full = add_rolling_feats(train_full, window=50)\n",
    "val_full = add_rolling_feats(val_full, window=50)\n",
    "test_df = add_rolling_feats(test_df, window=50)\n",
    "\n",
    "X_train = train_full.drop(columns='Class')\n",
    "Y_train = train_full['Class']\n",
    "X_val = val_full.drop(columns='Class')\n",
    "Y_val = val_full['Class']\n",
    "X_test = test_df.drop(columns='Class')\n",
    "Y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "train_dataset = DriveDataset(X_train, Y_train)\n",
    "val_dataset = DriveDataset(X_val, Y_val)\n",
    "test_dataset = DriveDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= 64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DriveModel(\n",
      "  (layer1): Linear(in_features=18, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (layer2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (output): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DriveModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(18, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.output = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.output(x)\n",
    "        return output\n",
    "\n",
    "model = DriveModel().to(device)\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "  Train Loss: 1.0991 | Train Acc: 0.3632\n",
      "  Val Loss: 1.0951 | Val Acc: 0.3647\n",
      "------------------------------\n",
      "Epoch [2/50]\n",
      "  Train Loss: 1.0953 | Train Acc: 0.3740\n",
      "  Val Loss: 1.0951 | Val Acc: 0.3647\n",
      "------------------------------\n",
      "Epoch [3/50]\n",
      "  Train Loss: 1.0897 | Train Acc: 0.3849\n",
      "  Val Loss: 1.0949 | Val Acc: 0.3676\n",
      "------------------------------\n",
      "Epoch [4/50]\n",
      "  Train Loss: 1.0873 | Train Acc: 0.4068\n",
      "  Val Loss: 1.0924 | Val Acc: 0.3941\n",
      "------------------------------\n",
      "Epoch [5/50]\n",
      "  Train Loss: 1.0812 | Train Acc: 0.4047\n",
      "  Val Loss: 1.0898 | Val Acc: 0.3824\n",
      "------------------------------\n",
      "Epoch [6/50]\n",
      "  Train Loss: 1.0776 | Train Acc: 0.4124\n",
      "  Val Loss: 1.0864 | Val Acc: 0.3897\n",
      "------------------------------\n",
      "Epoch [7/50]\n",
      "  Train Loss: 1.0657 | Train Acc: 0.4166\n",
      "  Val Loss: 1.0838 | Val Acc: 0.4029\n",
      "------------------------------\n",
      "Epoch [8/50]\n",
      "  Train Loss: 1.0650 | Train Acc: 0.4257\n",
      "  Val Loss: 1.0821 | Val Acc: 0.4000\n",
      "------------------------------\n",
      "Epoch [9/50]\n",
      "  Train Loss: 1.0621 | Train Acc: 0.4163\n",
      "  Val Loss: 1.0791 | Val Acc: 0.3750\n",
      "------------------------------\n",
      "Epoch [10/50]\n",
      "  Train Loss: 1.0523 | Train Acc: 0.4375\n",
      "  Val Loss: 1.0805 | Val Acc: 0.3941\n",
      "------------------------------\n",
      "Epoch [11/50]\n",
      "  Train Loss: 1.0548 | Train Acc: 0.4313\n",
      "  Val Loss: 1.0786 | Val Acc: 0.3853\n",
      "------------------------------\n",
      "Epoch [12/50]\n",
      "  Train Loss: 1.0519 | Train Acc: 0.4348\n",
      "  Val Loss: 1.0755 | Val Acc: 0.4029\n",
      "------------------------------\n",
      "Epoch [13/50]\n",
      "  Train Loss: 1.0503 | Train Acc: 0.4372\n",
      "  Val Loss: 1.0755 | Val Acc: 0.4029\n",
      "------------------------------\n",
      "Epoch [14/50]\n",
      "  Train Loss: 1.0456 | Train Acc: 0.4546\n",
      "  Val Loss: 1.0733 | Val Acc: 0.4015\n",
      "------------------------------\n",
      "Epoch [15/50]\n",
      "  Train Loss: 1.0417 | Train Acc: 0.4393\n",
      "  Val Loss: 1.0735 | Val Acc: 0.4206\n",
      "------------------------------\n",
      "Epoch [16/50]\n",
      "  Train Loss: 1.0437 | Train Acc: 0.4515\n",
      "  Val Loss: 1.0707 | Val Acc: 0.4235\n",
      "------------------------------\n",
      "Epoch [17/50]\n",
      "  Train Loss: 1.0441 | Train Acc: 0.4400\n",
      "  Val Loss: 1.0741 | Val Acc: 0.3985\n",
      "------------------------------\n",
      "Epoch [18/50]\n",
      "  Train Loss: 1.0434 | Train Acc: 0.4518\n",
      "  Val Loss: 1.0704 | Val Acc: 0.4162\n",
      "------------------------------\n",
      "Epoch [19/50]\n",
      "  Train Loss: 1.0333 | Train Acc: 0.4536\n",
      "  Val Loss: 1.0696 | Val Acc: 0.4191\n",
      "------------------------------\n",
      "Epoch [20/50]\n",
      "  Train Loss: 1.0333 | Train Acc: 0.4567\n",
      "  Val Loss: 1.0661 | Val Acc: 0.4221\n",
      "------------------------------\n",
      "Epoch [21/50]\n",
      "  Train Loss: 1.0327 | Train Acc: 0.4724\n",
      "  Val Loss: 1.0663 | Val Acc: 0.4162\n",
      "------------------------------\n",
      "Epoch [22/50]\n",
      "  Train Loss: 1.0350 | Train Acc: 0.4623\n",
      "  Val Loss: 1.0657 | Val Acc: 0.4221\n",
      "------------------------------\n",
      "Epoch [23/50]\n",
      "  Train Loss: 1.0268 | Train Acc: 0.4574\n",
      "  Val Loss: 1.0689 | Val Acc: 0.4206\n",
      "------------------------------\n",
      "Epoch [24/50]\n",
      "  Train Loss: 1.0298 | Train Acc: 0.4536\n",
      "  Val Loss: 1.0677 | Val Acc: 0.4294\n",
      "------------------------------\n",
      "Epoch [25/50]\n",
      "  Train Loss: 1.0278 | Train Acc: 0.4602\n",
      "  Val Loss: 1.0675 | Val Acc: 0.4279\n",
      "------------------------------\n",
      "Epoch [26/50]\n",
      "  Train Loss: 1.0274 | Train Acc: 0.4630\n",
      "  Val Loss: 1.0687 | Val Acc: 0.4176\n",
      "------------------------------\n",
      "Epoch [27/50]\n",
      "  Train Loss: 1.0186 | Train Acc: 0.4641\n",
      "  Val Loss: 1.0707 | Val Acc: 0.4191\n",
      "------------------------------\n",
      "Epoch [28/50]\n",
      "  Train Loss: 1.0169 | Train Acc: 0.4714\n",
      "  Val Loss: 1.0696 | Val Acc: 0.4250\n",
      "------------------------------\n",
      "Epoch [29/50]\n",
      "  Train Loss: 1.0298 | Train Acc: 0.4714\n",
      "  Val Loss: 1.0722 | Val Acc: 0.4353\n",
      "------------------------------\n",
      "Epoch [30/50]\n",
      "  Train Loss: 1.0149 | Train Acc: 0.4710\n",
      "  Val Loss: 1.0706 | Val Acc: 0.4368\n",
      "------------------------------\n",
      "Epoch [31/50]\n",
      "  Train Loss: 1.0146 | Train Acc: 0.4770\n",
      "  Val Loss: 1.0705 | Val Acc: 0.4441\n",
      "------------------------------\n",
      "Epoch [32/50]\n",
      "  Train Loss: 1.0112 | Train Acc: 0.4801\n",
      "  Val Loss: 1.0715 | Val Acc: 0.4294\n",
      "------------------------------\n",
      "Epoch [33/50]\n",
      "  Train Loss: 1.0140 | Train Acc: 0.4826\n",
      "  Val Loss: 1.0714 | Val Acc: 0.4338\n",
      "------------------------------\n",
      "Epoch [34/50]\n",
      "  Train Loss: 1.0091 | Train Acc: 0.4867\n",
      "  Val Loss: 1.0737 | Val Acc: 0.4382\n",
      "------------------------------\n",
      "Epoch [35/50]\n",
      "  Train Loss: 1.0072 | Train Acc: 0.4888\n",
      "  Val Loss: 1.0743 | Val Acc: 0.4294\n",
      "------------------------------\n",
      "Epoch [36/50]\n",
      "  Train Loss: 0.9982 | Train Acc: 0.4951\n",
      "  Val Loss: 1.0754 | Val Acc: 0.4368\n",
      "------------------------------\n",
      "Epoch [37/50]\n",
      "  Train Loss: 1.0069 | Train Acc: 0.4759\n",
      "  Val Loss: 1.0756 | Val Acc: 0.4338\n",
      "------------------------------\n",
      "Epoch [38/50]\n",
      "  Train Loss: 1.0048 | Train Acc: 0.4822\n",
      "  Val Loss: 1.0756 | Val Acc: 0.4441\n",
      "------------------------------\n",
      "Epoch [39/50]\n",
      "  Train Loss: 1.0016 | Train Acc: 0.4826\n",
      "  Val Loss: 1.0763 | Val Acc: 0.4279\n",
      "------------------------------\n",
      "Epoch [40/50]\n",
      "  Train Loss: 1.0118 | Train Acc: 0.4696\n",
      "  Val Loss: 1.0780 | Val Acc: 0.4206\n",
      "------------------------------\n",
      "Epoch [41/50]\n",
      "  Train Loss: 1.0093 | Train Acc: 0.4843\n",
      "  Val Loss: 1.0746 | Val Acc: 0.4235\n",
      "------------------------------\n",
      "Epoch [42/50]\n",
      "  Train Loss: 1.0044 | Train Acc: 0.5000\n",
      "  Val Loss: 1.0757 | Val Acc: 0.4235\n",
      "------------------------------\n",
      "Epoch [43/50]\n",
      "  Train Loss: 1.0019 | Train Acc: 0.4770\n",
      "  Val Loss: 1.0741 | Val Acc: 0.4191\n",
      "------------------------------\n",
      "Epoch [44/50]\n",
      "  Train Loss: 1.0066 | Train Acc: 0.4787\n",
      "  Val Loss: 1.0762 | Val Acc: 0.4235\n",
      "------------------------------\n",
      "Epoch [45/50]\n",
      "  Train Loss: 0.9985 | Train Acc: 0.4934\n",
      "  Val Loss: 1.0791 | Val Acc: 0.4147\n",
      "------------------------------\n",
      "Epoch [46/50]\n",
      "  Train Loss: 1.0028 | Train Acc: 0.4829\n",
      "  Val Loss: 1.0807 | Val Acc: 0.4103\n",
      "------------------------------\n",
      "Epoch [47/50]\n",
      "  Train Loss: 1.0040 | Train Acc: 0.4941\n",
      "  Val Loss: 1.0790 | Val Acc: 0.4206\n",
      "------------------------------\n",
      "Epoch [48/50]\n",
      "  Train Loss: 0.9991 | Train Acc: 0.4920\n",
      "  Val Loss: 1.0784 | Val Acc: 0.4147\n",
      "------------------------------\n",
      "Epoch [49/50]\n",
      "  Train Loss: 0.9922 | Train Acc: 0.4888\n",
      "  Val Loss: 1.0809 | Val Acc: 0.4235\n",
      "------------------------------\n",
      "Epoch [50/50]\n",
      "  Train Loss: 0.9953 | Train Acc: 0.4899\n",
      "  Val Loss: 1.0801 | Val Acc: 0.4176\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for input, labels in train_dataloader:\n",
    "        input, labels = input.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        batch_loss = criterion(output, labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "        _, predicted = torch.max(output,1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        train_correct  += correct\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        for input, labels in val_dataloader:\n",
    "            input, labels = input.to(device), labels.to(device)\n",
    "            output = model(input)\n",
    "            batch_loss = criterion(output, labels)\n",
    "            val_loss += batch_loss.item()\n",
    "            _, predicted = torch.max(output,1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            val_correct  += correct\n",
    "    \n",
    "    avg_train_loss = train_loss/len(train_dataloader)\n",
    "    avg_val_loss = val_loss/len(val_dataloader)\n",
    "    train_acc = train_correct/len(train_dataset)\n",
    "    val_acc = val_correct/len(val_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the data is hard to handle with for a beginner like me. Fed the output to Claude and it said LSTMs are the way to respect sequential data....which I shall learn eventually but not now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjecrtEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
